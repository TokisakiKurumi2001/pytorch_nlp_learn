{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiLSTM-CRF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o1R_1aQsMOw",
        "outputId": "d4a4c8da-b4f5-4d2a-b5d1-5ac63df7f3f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3c92e40090>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def argmax(vec):\n",
        "  return torch.argmax(vec, 1).item()\n",
        "\n",
        "def prepare_sequence(seq, to_idx):\n",
        "  idxs = [to_idx[w] for w in seq]\n",
        "  return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "def log_sum_exp(vec):\n",
        "  max_score = vec[0, argmax(vec)]\n",
        "  max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "  return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
      ],
      "metadata": {
        "id": "B36fPljSsVvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\""
      ],
      "metadata": {
        "id": "pw_-TOsQt9AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM_CRF(nn.Module):\n",
        "  def __init__(self, vocab_size, tag_to_idx, embed_dim, hidden_dim):\n",
        "    super(BiLSTM_CRF, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.vocab_size = vocab_size\n",
        "    self.tag_to_idx = tag_to_idx\n",
        "    self.tagset_size = len(tag_to_idx)\n",
        "\n",
        "    self.word_embed = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.lstm = nn.LSTM(embed_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
        "    self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "    # Matrix of transition parameters.  Entry i,j is the score of\n",
        "    # transitioning `to` i `from` j.\n",
        "    self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "    # These two statements enforce the constraint that we never transfer\n",
        "    # to the start tag and we never transfer from the stop tag\n",
        "    self.transitions.data[tag_to_idx[START_TAG], :] = -10000\n",
        "    self.transitions.data[:, tag_to_idx[STOP_TAG]] = -10000\n",
        "\n",
        "    self.hidden = self.init_hidden()\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.randn(2, 1, self.hidden_dim // 2), torch.randn(2, 1, self.hidden_dim // 2)\n",
        "\n",
        "  def _forward_alg(self, feats):\n",
        "    init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "    init_alphas[0][self.tag_to_idx[START_TAG]] = 0.\n",
        "    \n",
        "    forward_var = init_alphas\n",
        "\n",
        "    for feat in feats:\n",
        "      alphas_t = []\n",
        "      for next_tag in range(self.tagset_size):\n",
        "        emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
        "        trans_score = self.transitions[next_tag].view(1, -1)\n",
        "        next_tag_var = forward_var + trans_score + emit_score\n",
        "        alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "      forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "    terminal_var = forward_var + self.transitions[self.tag_to_idx[STOP_TAG]]\n",
        "    alpha = log_sum_exp(terminal_var)\n",
        "    return alpha\n",
        "\n",
        "  def _get_lstm_features(self, sentence):\n",
        "    self.hidden = self.init_hidden()\n",
        "    embeds = self.word_embed(sentence).view(len(sentence), 1, -1)\n",
        "    lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "    lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "    lstm_feats = self.hidden2tag(lstm_out)\n",
        "    return lstm_feats\n",
        "\n",
        "  def _score_sentence(self, feats, tags):\n",
        "    score = torch.zeros(1)\n",
        "    tags = torch.cat([torch.tensor([self.tag_to_idx[START_TAG]], dtype=torch.long), tags])\n",
        "    for i, feat in enumerate(feats):\n",
        "      score = score + self.transitions[tags[i+1], tags[i]] + feat[tags[i+1]]\n",
        "    score = score + self.transitions[self.tag_to_idx[STOP_TAG], tags[-1]]\n",
        "    return score\n",
        "\n",
        "  def _viterbi_decode(self, feats):\n",
        "    backpointers = []\n",
        "\n",
        "    init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "    init_vvars[0][self.tag_to_idx[START_TAG]] = 0\n",
        "\n",
        "    forward_var = init_vvars\n",
        "    for feat in feats:\n",
        "      bptrs_t = []\n",
        "      viterbivars_t = []\n",
        "\n",
        "      for next_tag in range(self.tagset_size):\n",
        "        next_tag_var = forward_var + self.transitions[next_tag]\n",
        "        best_tag_id = argmax(next_tag_var)\n",
        "        bptrs_t.append(best_tag_id)\n",
        "        viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "      forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "      backpointers.append(bptrs_t)\n",
        "\n",
        "    terminal_var = forward_var + self.transitions[self.tag_to_idx[STOP_TAG]]\n",
        "    best_tag_id = argmax(terminal_var)\n",
        "    path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "    best_path = [best_tag_id]\n",
        "    for bptrs_t in reversed(backpointers):\n",
        "      best_tag_id = bptrs_t[best_tag_id]\n",
        "      best_path.append(best_tag_id)\n",
        "    start = best_path.pop()\n",
        "    assert start == self.tag_to_idx[START_TAG]\n",
        "    best_path.reverse()\n",
        "    return path_score, best_path\n",
        "\n",
        "  def neg_log_likelihood(self, sentence, tags):\n",
        "    feats = self._get_lstm_features(sentence)\n",
        "    forward_score = self._forward_alg(feats)\n",
        "    gold_score = self._score_sentence(feats, tags)\n",
        "    return forward_score - gold_score\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    lstm_feats = self._get_lstm_features(sentence)\n",
        "    score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "    return score, tag_seq"
      ],
      "metadata": {
        "id": "kpqgdLeitG20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_DIM = 5\n",
        "HIDDEN_DIM = 4\n",
        "\n",
        "training_data = [(\n",
        "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
        "    \"B I I I O O O B I O O\".split()\n",
        "), (\n",
        "    \"georgia tech is a university in georgia\".split(),\n",
        "    \"B I O O O O B\".split()\n",
        ")]\n",
        "\n",
        "vocab = set([word for data in training_data for word in data[0]])\n",
        "word_to_idx = {word:idx for idx, word in enumerate(vocab)}\n",
        "tag_to_idx = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
        "idx_to_tag = list(tag_to_idx.keys())"
      ],
      "metadata": {
        "id": "LQlwSgJqmDwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def print_beauty_tag(sentence, tag_seq):\n",
        "  print(pd.DataFrame({'word': sentence, 'tag': [idx_to_tag[tag] for tag in tag_seq]}))"
      ],
      "metadata": {
        "id": "QfKPdN1nnFUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BiLSTM_CRF(len(word_to_idx), tag_to_idx, EMBED_DIM, HIDDEN_DIM)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_idx)\n",
        "    _, tag_seq = model(precheck_sent)\n",
        "    print_beauty_tag(training_data[0][0], tag_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnNM3bpAmpor",
        "outputId": "87855aa7-c426-45af-df15-25b6d026e07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           word tag\n",
            "0           the   B\n",
            "1          wall   O\n",
            "2        street   O\n",
            "3       journal   O\n",
            "4      reported   O\n",
            "5         today   O\n",
            "6          that   O\n",
            "7         apple   O\n",
            "8   corporation   O\n",
            "9          made   O\n",
            "10        money   O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(300):\n",
        "  for sentence, tags in training_data:\n",
        "    model.zero_grad()\n",
        "\n",
        "    sentence_in = prepare_sequence(sentence, word_to_idx)\n",
        "    targets = torch.tensor([tag_to_idx[t] for t in tags], dtype=torch.long)\n",
        "\n",
        "    loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "v8RGXgi6nlyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_idx)\n",
        "    _, tag_seq = model(precheck_sent)\n",
        "    print_beauty_tag(training_data[0][0], tag_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCggA5uun-Ud",
        "outputId": "1f9e3ac4-5ef8-4330-c85a-c35b87a7f9cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           word tag\n",
            "0           the   B\n",
            "1          wall   I\n",
            "2        street   I\n",
            "3       journal   I\n",
            "4      reported   O\n",
            "5         today   O\n",
            "6          that   O\n",
            "7         apple   B\n",
            "8   corporation   I\n",
            "9          made   O\n",
            "10        money   O\n"
          ]
        }
      ]
    }
  ]
}