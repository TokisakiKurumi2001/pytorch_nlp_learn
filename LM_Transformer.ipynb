{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LM_Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import library"
      ],
      "metadata": {
        "id": "XZQb_kaReLKZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPMhaUBAdujG"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter()"
      ],
      "metadata": {
        "id": "fBnCwenc0bqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the model"
      ],
      "metadata": {
        "id": "1u0x2K7ZeKA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(max_len, 1, d_model)\n",
        "    pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "    \"\"\"\n",
        "    x = x + self.pe[:x.size(0)]\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "vti3BQ6EeNjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLM(nn.Module):\n",
        "  def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int,\n",
        "               dropout: float = 0.5):\n",
        "    super(TransformerLM, self).__init__()\n",
        "    self.model_type = 'Transformer'\n",
        "    self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "    encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "    self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "    self.encoder = nn.Embedding(ntoken, d_model)\n",
        "    self.d_model = d_model\n",
        "    self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self) -> None:\n",
        "    initrange = 0.1\n",
        "    self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "  def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        src: Tensor, shape [seq_len, batch_size]\n",
        "        src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "    Returns:\n",
        "        output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "    \"\"\"\n",
        "    src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "    src = self.pos_encoder(src)\n",
        "    output = self.transformer_encoder(src, src_mask)\n",
        "    output = self.decoder(output)\n",
        "    return output\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "  \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "  return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ],
      "metadata": {
        "id": "2ch2q_aLfcYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset and batch data"
      ],
      "metadata": {
        "id": "HnHpsn-yjoWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "  \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "  data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "  \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "  that wouldn't cleanly fit.\n",
        "\n",
        "  Args:\n",
        "      data: Tensor, shape [N]\n",
        "      bsz: int, batch size\n",
        "\n",
        "  Returns:\n",
        "      Tensor of shape [N // bsz, bsz]\n",
        "  \"\"\"\n",
        "  seq_len = data.size(0) // bsz\n",
        "  data = data[:seq_len * bsz]\n",
        "  data = data.view(bsz, seq_len).t().contiguous()\n",
        "  return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size) # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieum_87th2H0",
        "outputId": "39ea7183-2256-497c-9e78-c23feff18a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.48M/4.48M [00:00<00:00, 19.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate input and target sequence"
      ],
      "metadata": {
        "id": "KOE-DJcTjlPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bptt = 35\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "  \"\"\"\n",
        "  Args:\n",
        "      source: Tensor, shape [full_seq_len, batch_size]\n",
        "      i: int\n",
        "\n",
        "  Returns:\n",
        "      tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "      target has shape [seq_len * batch_size]\n",
        "  \"\"\"\n",
        "  seq_len = min(bptt, len(source) - 1 - i)\n",
        "  data = source[i:i+seq_len]\n",
        "  target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "  return data, target"
      ],
      "metadata": {
        "id": "XZrKwkvBjtwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initiate an instance"
      ],
      "metadata": {
        "id": "wSG9iXMVkJ4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ntokens = len(vocab)\n",
        "emsize = 200\n",
        "d_hid = 200\n",
        "nlayers = 2\n",
        "nhead = 2\n",
        "dropout = 0.2\n",
        "model = TransformerLM(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ],
      "metadata": {
        "id": "VwsGeax0kLks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the model"
      ],
      "metadata": {
        "id": "-Q2igmTOkpCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "  model.train() # turn on train mode\n",
        "  total_loss = 0.\n",
        "  log_interval = 200\n",
        "  start_time = time.time()\n",
        "  src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "  num_batches = len(train_data) // bptt\n",
        "  for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "    data, targets = get_batch(train_data, i)\n",
        "    batch_size = data.size(0)\n",
        "    if batch_size != bptt: # only on last batch\n",
        "      src_mask = src_mask[:batch_size, :batch_size]\n",
        "    output = model(data, src_mask)\n",
        "    loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    if batch % log_interval == 0 and batch > 0:\n",
        "      lr = scheduler.get_last_lr()[0]\n",
        "      ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "      cur_loss = total_loss / log_interval\n",
        "      ppl = math.exp(cur_loss)\n",
        "      print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "      total_loss = 0\n",
        "      start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "  model.eval() # turn on evaluation mode\n",
        "  total_loss = 0.\n",
        "  src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "  with torch.no_grad():\n",
        "    for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "      data, targets = get_batch(eval_data, i)\n",
        "      batch_size = data.size(0)\n",
        "      if batch_size != bptt:\n",
        "        src_mask = src_mask[:batch_size, :batch_size]\n",
        "      output = model(data, src_mask)\n",
        "      output_flat = output.view(-1, ntokens)\n",
        "      total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "  return total_loss / (len(eval_data) - 1)"
      ],
      "metadata": {
        "id": "NkB657R3kqV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 3\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  epoch_start_time = time.time()\n",
        "  train(model)\n",
        "  val_loss = evaluate(model, val_data)\n",
        "  val_ppl = math.exp(val_loss)\n",
        "  train_loss = evaluate(model, train_data)\n",
        "  train_ppl = math.exp(train_loss)\n",
        "  writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
        "  writer.add_scalar(\"Perplexity/train\", train_ppl, epoch)\n",
        "  writer.add_scalar(\"Loss/valid\", val_loss, epoch)\n",
        "  writer.add_scalar(\"Perplexity/valid\", val_ppl, epoch)\n",
        "  elapsed = time.time() - epoch_start_time\n",
        "  print('-' * 89)\n",
        "  print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "  print('-' * 89)\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    best_model = copy.deepcopy(model)\n",
        "  scheduler.step()"
      ],
      "metadata": {
        "id": "Z391JMWNnLTm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab3db06a-6b57-490e-c0b7-a9ba5e08093d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 36.52 | loss  8.14 | ppl  3413.01\n",
            "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 35.11 | loss  6.87 | ppl   966.28\n",
            "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 35.18 | loss  6.43 | ppl   621.48\n",
            "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 35.20 | loss  6.30 | ppl   543.94\n",
            "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 36.39 | loss  6.19 | ppl   485.49\n",
            "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 36.05 | loss  6.15 | ppl   469.82\n",
            "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 35.46 | loss  6.11 | ppl   451.39\n",
            "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 35.39 | loss  6.11 | ppl   449.04\n",
            "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 35.41 | loss  6.02 | ppl   413.34\n",
            "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 36.22 | loss  6.02 | ppl   410.07\n",
            "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 35.47 | loss  5.90 | ppl   365.36\n",
            "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 35.44 | loss  5.97 | ppl   392.64\n",
            "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 36.15 | loss  5.96 | ppl   386.89\n",
            "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 37.39 | loss  5.89 | ppl   359.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 108.53s | valid loss  5.82 | valid ppl   335.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 35.52 | loss  5.87 | ppl   353.08\n",
            "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 35.47 | loss  5.86 | ppl   351.09\n",
            "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 35.40 | loss  5.66 | ppl   288.25\n",
            "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 35.35 | loss  5.71 | ppl   301.07\n",
            "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 35.38 | loss  5.66 | ppl   287.34\n",
            "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 35.35 | loss  5.68 | ppl   294.22\n",
            "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 35.40 | loss  5.69 | ppl   296.87\n",
            "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 35.34 | loss  5.72 | ppl   303.64\n",
            "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 35.51 | loss  5.66 | ppl   286.29\n",
            "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 35.38 | loss  5.67 | ppl   289.74\n",
            "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 35.39 | loss  5.56 | ppl   259.42\n",
            "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 35.51 | loss  5.64 | ppl   282.65\n",
            "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 35.39 | loss  5.64 | ppl   281.56\n",
            "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 35.42 | loss  5.58 | ppl   264.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 107.56s | valid loss  5.66 | valid ppl   286.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 35.77 | loss  5.60 | ppl   270.74\n",
            "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 35.73 | loss  5.62 | ppl   277.04\n",
            "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 35.66 | loss  5.42 | ppl   226.78\n",
            "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 35.72 | loss  5.48 | ppl   240.19\n",
            "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 35.80 | loss  5.43 | ppl   229.27\n",
            "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 35.89 | loss  5.48 | ppl   238.93\n",
            "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 35.88 | loss  5.49 | ppl   242.56\n",
            "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 35.48 | loss  5.52 | ppl   249.75\n",
            "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 35.43 | loss  5.47 | ppl   238.24\n",
            "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 35.35 | loss  5.49 | ppl   241.05\n",
            "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 35.50 | loss  5.36 | ppl   212.22\n",
            "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 35.57 | loss  5.46 | ppl   235.52\n",
            "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 35.43 | loss  5.47 | ppl   237.62\n",
            "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 35.38 | loss  5.40 | ppl   221.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 108.01s | valid loss  5.57 | valid ppl   261.71\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "writer.flush()\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "lGyaawJt04aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "id": "crgYS2_ZntRk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5389509-fc52-4391-bf1a-fc35c1b552e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.48 | test ppl   239.17\n",
            "=========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "sZWwAeYw2nhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "RwL_KuNn1RHk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}