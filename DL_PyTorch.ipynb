{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intro Deep Learning"
      ],
      "metadata": {
        "id": "HaJ4-qwSKYxg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPoidmZ9JjFJ",
        "outputId": "f1355cab-c70f-427e-baec-9670dce2bd0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5f0361aa10>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin = nn.Linear(5, 3)# maps from R^5 to R^3, parameters A, b\n",
        "# data is 2x5. A maps from 5 to 3.. can we map \"data\" under A?\n",
        "data = torch.randn(2, 5)\n",
        "print(lin(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6aImthJJ6I5",
        "outputId": "4d6dccae-4f08-4e18-a2b5-319d3ab1569e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1755, -0.3268, -0.5069],\n",
            "        [-0.6602,  0.2260,  0.1089]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.randn(2, 2)\n",
        "print(data)\n",
        "print(F.relu(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_Mu3RX6KV6c",
        "outputId": "4e08a4cc-5759-4570-f123-4f11ed351e2f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.5404, -2.2102],\n",
            "        [ 2.1130, -0.0040]])\n",
            "tensor([[0.0000, 0.0000],\n",
            "        [2.1130, 0.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.randn(5)\n",
        "print(data)\n",
        "print(F.softmax(data, dim=0))\n",
        "print(F.softmax(data, dim=0).sum())\n",
        "print(F.log_softmax(data, dim=0))\n",
        "print(F.log_softmax(data, dim=0).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-N_fnV2KgAq",
        "outputId": "b653c951-5cbe-4e83-8e41-aa63cfb0a002"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.3800, -1.3505,  0.3455,  0.5046,  1.8213])\n",
            "tensor([0.2948, 0.0192, 0.1048, 0.1228, 0.4584])\n",
            "tensor(1.)\n",
            "tensor([-1.2214, -3.9519, -2.2560, -2.0969, -0.7801])\n",
            "tensor(-10.3063)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Declaring Model"
      ],
      "metadata": {
        "id": "-uWlenIXLonw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
        "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
        "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
        "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
        "\n",
        "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
        "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
        "\n",
        "word_to_idx = {}\n",
        "for sent, _ in data + test_data:\n",
        "  for word in sent:\n",
        "    if word not in word_to_idx:\n",
        "      word_to_idx[word] = len(word_to_idx)\n",
        "print(word_to_idx)\n",
        "\n",
        "VOCAB_SIZE = len(word_to_idx)\n",
        "NUM_LABELS = 2\n",
        "\n",
        "class BoWClassifier(nn.Module):\n",
        "  def __init__(self, num_labels, vocab_size):\n",
        "    super(BoWClassifier, self).__init__()\n",
        "\n",
        "    self.linear = nn.Linear(vocab_size, num_labels)\n",
        "\n",
        "  def forward(self, bow_vec):\n",
        "    return F.log_softmax(self.linear(bow_vec), dim=1)\n",
        "\n",
        "def make_bow_vector(sentence, word_to_idx):\n",
        "  vec = torch.zeros(len(word_to_idx))\n",
        "  for word in sentence:\n",
        "    vec[word_to_idx[word]] = vec[word_to_idx[word]] + 1\n",
        "  return vec.view(1, -1)\n",
        "\n",
        "def make_target(label, label_to_idx):\n",
        "  return torch.LongTensor([label_to_idx[label]])\n",
        "\n",
        "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
        "for param in model.parameters():\n",
        "  print(param)\n",
        "\n",
        "# Take if the model works\n",
        "with torch.no_grad():\n",
        "  sample = data[0]\n",
        "  bow_vector = make_bow_vector(sample[0], word_to_idx)\n",
        "  log_probs = model(bow_vector)\n",
        "  print(log_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QWO-DCmLp5e",
        "outputId": "f8a3086a-7e5a-447a-c0c6-3424bcc7c716"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n",
            "Parameter containing:\n",
            "tensor([[ 0.1194,  0.0609, -0.1268,  0.1274,  0.1191,  0.1739, -0.1099, -0.0323,\n",
            "         -0.0038,  0.0286, -0.1488, -0.1392,  0.1067, -0.0460,  0.0958,  0.0112,\n",
            "          0.0644,  0.0431,  0.0713,  0.0972, -0.1816,  0.0987, -0.1379, -0.1480,\n",
            "          0.0119, -0.0334],\n",
            "        [ 0.1152, -0.1136, -0.1743,  0.1427, -0.0291,  0.1103,  0.0630, -0.1471,\n",
            "          0.0394,  0.0471, -0.1313, -0.0931,  0.0669,  0.0351, -0.0834, -0.0594,\n",
            "          0.1796, -0.0363,  0.1106,  0.0849, -0.1268, -0.1668,  0.1882,  0.0102,\n",
            "          0.1344,  0.0406]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0631, 0.1465], requires_grad=True)\n",
            "tensor([[-0.5378, -0.8771]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_idx = {\"SPANISH\": 0, \"ENGLISH\": 1}"
      ],
      "metadata": {
        "id": "rBQjuux5NLpA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for instance, label in test_data:\n",
        "    bow_vec = make_bow_vector(instance, word_to_idx)\n",
        "    log_probs = model(bow_vec)\n",
        "    print(log_probs)\n",
        "\n",
        "# Print the matrix column corresponding to \"creo\"\n",
        "print(next(model.parameters())[:, word_to_idx[\"creo\"]])\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "for epoch in range(100):\n",
        "  for instance, label in data:\n",
        "    # Step 1. Remember that PyTorch accumulates gradients.\n",
        "    # We need to clear them out before each instance\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Step 2. Make our BOW vector and also we must wrap the target in a\n",
        "    # Tensor as an integer. For example, if the target is SPANISH, then\n",
        "    # we wrap the integer 0. The loss function then knows that the 0th\n",
        "    # element of the log probabilities is the log probability\n",
        "    # corresponding to SPANISH\n",
        "    bow_vec = make_bow_vector(instance, word_to_idx)\n",
        "    target = make_target(label, label_to_idx)\n",
        "\n",
        "    # Step 3. Run our forward pass.\n",
        "    log_probs = model(bow_vec)\n",
        "    # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "    # calling optimizer.step()\n",
        "    loss = loss_fn(log_probs, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for instance, label in test_data:\n",
        "    bow_vec = make_bow_vector(instance, word_to_idx)\n",
        "    log_probs = model(bow_vec)\n",
        "    print(log_probs)\n",
        "\n",
        "# Index corresponding to Spanish goes up, English goes down!\n",
        "print(next(model.parameters())[:, word_to_idx[\"creo\"]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrCKhYrjNPvn",
        "outputId": "1c9e6547-a0a4-4528-d0b6-b0a8428e06ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.9297, -0.5020]])\n",
            "tensor([[-0.6388, -0.7506]])\n",
            "tensor([-0.1488, -0.1313], grad_fn=<SelectBackward0>)\n",
            "tensor([[-0.2093, -1.6669]])\n",
            "tensor([[-2.5330, -0.0828]])\n",
            "tensor([ 0.2803, -0.5605], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    }
  ]
}